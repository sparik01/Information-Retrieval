{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval: Programming Assignment \\#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sheetal Parikh\n",
    "EN.605.744.81<br>\n",
    "September 7, 2021\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for notebook\n",
    "import os \n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "#import num2words\n",
    "#pip install num2words\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# change the current directory \n",
    "# to specified directory \n",
    "os.chdir(r\"C:\\Users\\Sheetal\\Documents\\Sheetal\\datasets\") \n",
    "\n",
    "#checking current directory\n",
    "#print(os.getcwd() + \"\\n\")\n",
    "\n",
    "#Read in file\n",
    "#path1 = 'datasets/headlines.txt'\n",
    "#path2 = 'datasets/yelp.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the raw text file includes HTML headings and tags - the function below will remove these tags\n",
    "\n",
    "def cleanHtmlTags(raw_text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    fixedText = re.sub(clean, '', raw_text)\n",
    "    return fixedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will open/close read the text file, clean the HTML tags, and copy the resulting data into a list\n",
    "\n",
    "def openFile(textFile):\n",
    "    content = []\n",
    "    with open(textFile, \"r\") as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            cleantext = cleanHtmlTags(text)\n",
    "            content.append(cleantext)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the list into a dataframe so that it is easier to use\n",
    "\n",
    "def createDataFrame(data):\n",
    "    #removing extra spacing\n",
    "    data = list(filter(None,data))\n",
    "    \n",
    "    #the dictionary column will contain the original wording of each document/paragraph\n",
    "    df = pd.DataFrame(np.array(data), columns = ['dictionary'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will convert contractions into it's original word combination\n",
    "\n",
    "def removeContractions(s):\n",
    "    s = re.sub(r\"can\\'t\", \"can not\",s)\n",
    "    s = re.sub(r\"couldn't\", \"can not\",s)\n",
    "    s = re.sub(r\"won't\", \"will not\",s)\n",
    "    s = re.sub(r\"\\'ll\", \" will\", s)\n",
    "    s = re.sub(r\"wouldn't\", \"would not\",s)\n",
    "    s = re.sub(r\"\\'d\", \"would\",s)\n",
    "    s = re.sub(r\"\\'s\", \" is\", s)\n",
    "    s = re.sub(r\"\\'re\", \" are\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have\", s)\n",
    "    s = re.sub(r\"\\'m\", \" am\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def stemming(data):\n",
    "#    stemmer= PorterStemmer()\n",
    "#    \n",
    "#    tokens = word_tokenize(str(data))\n",
    "#    new_text = \"\"\n",
    "#    for w in tokens:\n",
    "#        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "#    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data - making text all lowercase, removing contractions, removing punctuation, lemmatizing text\n",
    "## each preprocessing step will add a column to the dataframe showing the updated text\n",
    "###there is a column showing the text if stop words were removed - this column is not used other than for calculating\n",
    "####the dictionary percentage in the function below\n",
    "\n",
    "def preprocess(df):\n",
    "    #lowercase\n",
    "    df['fix_lowercase'] = df['dictionary'].apply(lambda x: \" \".join(word.lower() for word in x.split()))\n",
    "    \n",
    "    #contractions\n",
    "    df['fix_contractions'] = df['fix_lowercase'].apply(lambda x:removeContractions(x))\n",
    "    \n",
    "    #punctuation\n",
    "    df['fix_punctuation'] = df['fix_contractions'].str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    #lemmatized\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemmatized'] = df['fix_punctuation'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    #df['num_stopwords'] = df['lemmatized'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n",
    "    df['fix_stopwords'] = df['lemmatized'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds the analysis steps as columns to the dataframe\n",
    "##columns will contain the word count(after preprocessing), number of stopwords, number of unique words, and percentage\n",
    "###of words in the document that are dictionary words(or unique words divided by the word count)\n",
    "\n",
    "def analyze(df):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    #column for number of stopwords\n",
    "    df['num_stopwords'] = df['lemmatized'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n",
    "    \n",
    "    #word count - using the final preprocessed text\n",
    "    words = df['lemmatized'].str.lower().str.split()\n",
    "    df['word_count'] = words.apply(len)\n",
    "    \n",
    "    #unique word count\n",
    "    noStopWords = df['fix_stopwords'].str.lower().str.split()\n",
    "    df['unique_words'] = noStopWords.apply(set).apply(len)\n",
    "    \n",
    "    #dictionary percentage or the % of unique words in the document\n",
    "    percent_column = df['unique_words'] / df['word_count']\n",
    "    df['dictionary_percentage'] = percent_column\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequentWords(df):\n",
    "    #getting a list of the total counts of each word in the preprocessed text - in descending order\n",
    "    s = pd.Series(\" \".join(df['lemmatized']).split()).value_counts()\n",
    "    df1 = s.to_frame()\n",
    "    \n",
    "    #extracting the word from the index\n",
    "    index = df1.index\n",
    "    allWords = list(index)\n",
    "\n",
    "    #extracting the total word count\n",
    "    counts = df1.iloc[:,0]\n",
    "    countsList = list(counts)\n",
    "    \n",
    "    #getting the document set\n",
    "    set = df['lemmatized']\n",
    "    \n",
    "    #creating a dictionary of the words and word count which is then converted to a dataframe\n",
    "    dict = {'Word': allWords, 'Word_Count': countsList} \n",
    "    df_dict = pd.DataFrame(dict)\n",
    "    \n",
    "    #calculating the number of documents each word is found in and saving the word and document count to dictionary DocFreq\n",
    "    wordsets = [ frozenset(document.split(' ')) for document in set ]\n",
    "    DocFreq = {}\n",
    "    for i in range(len(wordsets)):\n",
    "        tokens = wordsets[i]\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                DocFreq[w].add(i)\n",
    "            except:\n",
    "                DocFreq[w] = {i}\n",
    "    \n",
    "    for i in DocFreq:\n",
    "        DocFreq[i] = len(DocFreq[i])\n",
    "    \n",
    "    #organizing document frequency output into a dataframe\n",
    "    df_DocFreq = pd.DataFrame(list(DocFreq.items()), columns=['Word', 'Document_Frequency'])\n",
    "    df_output = df_DocFreq.sort_values(by='Document_Frequency', ascending = False)\n",
    "    \n",
    "    #merging results to the dataframe of total word counts using the word as the id\n",
    "    df_final = pd.merge(df_output, df_dict, left_on = 'Word', right_on = 'Word', how ='left')\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying other stats of documents: total words, total unique words, total paragraphs, 500th word, 1000th word,5000th word\n",
    "\n",
    "def calcStats(df):\n",
    "    #total paragraphs/dictionaries processed\n",
    "    total_paragraphs = len(df.index)\n",
    "    \n",
    "    #total words after proprocessing\n",
    "    total_words = df['word_count'].sum()\n",
    "    \n",
    "    #total unique words\n",
    "    total_unique = df['unique_words'].sum()\n",
    "    \n",
    "    #500th word\n",
    "    fivehund = pd.Series(\" \".join(df['lemmatized']).split()).value_counts()[499:500].index[0]\n",
    "    df_500 = df[df['lemmatized'].str.contains(fivehund)]\n",
    "    \n",
    "    #1000th word\n",
    "    thousand = pd.Series(\" \".join(df['lemmatized']).split()).value_counts()[999:1000].index[0]\n",
    "    df_1000 = df[df['lemmatized'].str.contains(thousand)]\n",
    "    \n",
    "    #5000th word\n",
    "    fivethou = pd.Series(\" \".join(df['lemmatized']).split()).value_counts()[4999:5000].index[0]\n",
    "    df_5000 = df[df['lemmatized'].str.contains(fivethou)]\n",
    "    \n",
    "    #display results\n",
    "    print(f'Total Paragraphs: {total_paragraphs}')\n",
    "    print(f'Total Words: {total_words}')\n",
    "    print(f'Total Unique Words: {total_unique}')\n",
    "    print(f'500th Most Frequent Word: {fivehund}, # of times Seen: {len(df_500)}')\n",
    "    print(f'1000th Most Frequent Word: {thousand}, # of times Seen: {len(df_1000)}')\n",
    "    print(f'5000th Most Frequent Word: {fivethou}, # of times Seen: {len(df_5000)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import display\n",
    "#def displayMostFrequent(df):\n",
    "#    with pd.option_context('display.max_rows', 100, 'display.max_columns', None):\n",
    "#        print(\"The 100 most frequent words: \")\n",
    "#        display(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the word, document frequency and word count filtered by a specific document frequency\n",
    "def documentFreq(df, x):\n",
    "    df_mask=df['Document_Frequency']== x\n",
    "    filtered_df = df[df_mask]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = openFile(\"yelp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = createDataFrame(content)\n",
    "df_yelp = preprocess(df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dictionary</th>\n",
       "      <th>fix_lowercase</th>\n",
       "      <th>fix_contractions</th>\n",
       "      <th>fix_punctuation</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>fix_stopwords</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>dictionary_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seen this restaurant on 25 best places in Pitt...</td>\n",
       "      <td>seen this restaurant on 25 best places in pitt...</td>\n",
       "      <td>seen this restaurant on 25 best places in pitt...</td>\n",
       "      <td>seen this restaurant on 25 best places in pitt...</td>\n",
       "      <td>seen this restaurant on 25 best place in pitts...</td>\n",
       "      <td>seen restaurant 25 best place pittsburgh rick ...</td>\n",
       "      <td>32</td>\n",
       "      <td>68</td>\n",
       "      <td>34</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grew up near here. the family would always go ...</td>\n",
       "      <td>grew up near here. the family would always go ...</td>\n",
       "      <td>grew up near here. the family would always go ...</td>\n",
       "      <td>grew up near here the family would always go o...</td>\n",
       "      <td>grew up near here the family would always go o...</td>\n",
       "      <td>grew near family would always go month stopped...</td>\n",
       "      <td>27</td>\n",
       "      <td>52</td>\n",
       "      <td>23</td>\n",
       "      <td>0.442308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have never seen a restaurant that has a frow...</td>\n",
       "      <td>i have never seen a restaurant that has a frow...</td>\n",
       "      <td>i have never seen a restaurant that has a frow...</td>\n",
       "      <td>i have never seen a restaurant that has a frow...</td>\n",
       "      <td>i have never seen a restaurant that ha a frown...</td>\n",
       "      <td>never seen restaurant ha frowning brownie aka ...</td>\n",
       "      <td>163</td>\n",
       "      <td>338</td>\n",
       "      <td>131</td>\n",
       "      <td>0.387574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stick to basics and this is the best place in ...</td>\n",
       "      <td>stick to basics and this is the best place in ...</td>\n",
       "      <td>stick to basics and this is the best place in ...</td>\n",
       "      <td>stick to basics and this is the best place in ...</td>\n",
       "      <td>stick to basic and this is the best place in o...</td>\n",
       "      <td>stick basic best place around burgh first time...</td>\n",
       "      <td>73</td>\n",
       "      <td>156</td>\n",
       "      <td>73</td>\n",
       "      <td>0.467949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like the food at Denny's more than meals ser...</td>\n",
       "      <td>i like the food at denny's more than meals ser...</td>\n",
       "      <td>i like the food at denny is more than meals se...</td>\n",
       "      <td>i like the food at denny is more than meals se...</td>\n",
       "      <td>i like the food at denny is more than meal ser...</td>\n",
       "      <td>like food denny meal served pricier restaurant...</td>\n",
       "      <td>35</td>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>Well, it's clear they are newly opened and the...</td>\n",
       "      <td>well, it's clear they are newly opened and the...</td>\n",
       "      <td>well, it is clear they are newly opened and th...</td>\n",
       "      <td>well it is clear they are newly opened and the...</td>\n",
       "      <td>well it is clear they are newly opened and the...</td>\n",
       "      <td>well clear newly opened staff working figure 2...</td>\n",
       "      <td>85</td>\n",
       "      <td>164</td>\n",
       "      <td>59</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>This place sucks! Wtf is this?! A fish Taco th...</td>\n",
       "      <td>this place sucks! wtf is this?! a fish taco th...</td>\n",
       "      <td>this place sucks! wtf is this?! a fish taco th...</td>\n",
       "      <td>this place sucks wtf is this a fish taco that ...</td>\n",
       "      <td>this place suck wtf is this a fish taco that i...</td>\n",
       "      <td>place suck wtf fish taco solely fish tortilla ...</td>\n",
       "      <td>133</td>\n",
       "      <td>246</td>\n",
       "      <td>82</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>I hope this place is just experiencing some hi...</td>\n",
       "      <td>i hope this place is just experiencing some hi...</td>\n",
       "      <td>i hope this place is just experiencing some hi...</td>\n",
       "      <td>i hope this place is just experiencing some hi...</td>\n",
       "      <td>i hope this place is just experiencing some hi...</td>\n",
       "      <td>hope place experiencing hiccup since openedbut...</td>\n",
       "      <td>51</td>\n",
       "      <td>121</td>\n",
       "      <td>59</td>\n",
       "      <td>0.487603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8890</th>\n",
       "      <td>Meh. I was less than impressed by my meal here...</td>\n",
       "      <td>meh. i was less than impressed by my meal here...</td>\n",
       "      <td>meh. i was less than impressed by my meal here...</td>\n",
       "      <td>meh i was less than impressed by my meal here ...</td>\n",
       "      <td>meh i wa le than impressed by my meal here it ...</td>\n",
       "      <td>meh wa le impressed meal cute concept sure goi...</td>\n",
       "      <td>61</td>\n",
       "      <td>123</td>\n",
       "      <td>53</td>\n",
       "      <td>0.430894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8891</th>\n",
       "      <td>Is give this place 0 stars if I could.  Invent...</td>\n",
       "      <td>is give this place 0 stars if i could. invent ...</td>\n",
       "      <td>is give this place 0 stars if i could. invent ...</td>\n",
       "      <td>is give this place 0 stars if i could invent h...</td>\n",
       "      <td>is give this place 0 star if i could invent he...</td>\n",
       "      <td>give place 0 star could invent buddy work food...</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>27</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8892 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             dictionary  \\\n",
       "0     Seen this restaurant on 25 best places in Pitt...   \n",
       "1     Grew up near here. the family would always go ...   \n",
       "2     I have never seen a restaurant that has a frow...   \n",
       "3     Stick to basics and this is the best place in ...   \n",
       "4     I like the food at Denny's more than meals ser...   \n",
       "...                                                 ...   \n",
       "8887  Well, it's clear they are newly opened and the...   \n",
       "8888  This place sucks! Wtf is this?! A fish Taco th...   \n",
       "8889  I hope this place is just experiencing some hi...   \n",
       "8890  Meh. I was less than impressed by my meal here...   \n",
       "8891  Is give this place 0 stars if I could.  Invent...   \n",
       "\n",
       "                                          fix_lowercase  \\\n",
       "0     seen this restaurant on 25 best places in pitt...   \n",
       "1     grew up near here. the family would always go ...   \n",
       "2     i have never seen a restaurant that has a frow...   \n",
       "3     stick to basics and this is the best place in ...   \n",
       "4     i like the food at denny's more than meals ser...   \n",
       "...                                                 ...   \n",
       "8887  well, it's clear they are newly opened and the...   \n",
       "8888  this place sucks! wtf is this?! a fish taco th...   \n",
       "8889  i hope this place is just experiencing some hi...   \n",
       "8890  meh. i was less than impressed by my meal here...   \n",
       "8891  is give this place 0 stars if i could. invent ...   \n",
       "\n",
       "                                       fix_contractions  \\\n",
       "0     seen this restaurant on 25 best places in pitt...   \n",
       "1     grew up near here. the family would always go ...   \n",
       "2     i have never seen a restaurant that has a frow...   \n",
       "3     stick to basics and this is the best place in ...   \n",
       "4     i like the food at denny is more than meals se...   \n",
       "...                                                 ...   \n",
       "8887  well, it is clear they are newly opened and th...   \n",
       "8888  this place sucks! wtf is this?! a fish taco th...   \n",
       "8889  i hope this place is just experiencing some hi...   \n",
       "8890  meh. i was less than impressed by my meal here...   \n",
       "8891  is give this place 0 stars if i could. invent ...   \n",
       "\n",
       "                                        fix_punctuation  \\\n",
       "0     seen this restaurant on 25 best places in pitt...   \n",
       "1     grew up near here the family would always go o...   \n",
       "2     i have never seen a restaurant that has a frow...   \n",
       "3     stick to basics and this is the best place in ...   \n",
       "4     i like the food at denny is more than meals se...   \n",
       "...                                                 ...   \n",
       "8887  well it is clear they are newly opened and the...   \n",
       "8888  this place sucks wtf is this a fish taco that ...   \n",
       "8889  i hope this place is just experiencing some hi...   \n",
       "8890  meh i was less than impressed by my meal here ...   \n",
       "8891  is give this place 0 stars if i could invent h...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0     seen this restaurant on 25 best place in pitts...   \n",
       "1     grew up near here the family would always go o...   \n",
       "2     i have never seen a restaurant that ha a frown...   \n",
       "3     stick to basic and this is the best place in o...   \n",
       "4     i like the food at denny is more than meal ser...   \n",
       "...                                                 ...   \n",
       "8887  well it is clear they are newly opened and the...   \n",
       "8888  this place suck wtf is this a fish taco that i...   \n",
       "8889  i hope this place is just experiencing some hi...   \n",
       "8890  meh i wa le than impressed by my meal here it ...   \n",
       "8891  is give this place 0 star if i could invent he...   \n",
       "\n",
       "                                          fix_stopwords  num_stopwords  \\\n",
       "0     seen restaurant 25 best place pittsburgh rick ...             32   \n",
       "1     grew near family would always go month stopped...             27   \n",
       "2     never seen restaurant ha frowning brownie aka ...            163   \n",
       "3     stick basic best place around burgh first time...             73   \n",
       "4     like food denny meal served pricier restaurant...             35   \n",
       "...                                                 ...            ...   \n",
       "8887  well clear newly opened staff working figure 2...             85   \n",
       "8888  place suck wtf fish taco solely fish tortilla ...            133   \n",
       "8889  hope place experiencing hiccup since openedbut...             51   \n",
       "8890  meh wa le impressed meal cute concept sure goi...             61   \n",
       "8891  give place 0 star could invent buddy work food...             22   \n",
       "\n",
       "      word_count  unique_words  dictionary_percentage  \n",
       "0             68            34               0.500000  \n",
       "1             52            23               0.442308  \n",
       "2            338           131               0.387574  \n",
       "3            156            73               0.467949  \n",
       "4             75            37               0.493333  \n",
       "...          ...           ...                    ...  \n",
       "8887         164            59               0.359756  \n",
       "8888         246            82               0.333333  \n",
       "8889         121            59               0.487603  \n",
       "8890         123            53               0.430894  \n",
       "8891          50            27               0.540000  \n",
       "\n",
       "[8892 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp = analyze(df_yelp)\n",
    "df_yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above summarizes much of the data that will be used for the calculations below.  The dictionary column is the original text and the subsequent 5 columns display the text changes after applying the normalization technique in the title. The lemmatized text was used for the calculations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paragraphs: 8892\n",
      "Total Words: 1286762\n",
      "Total Unique Words: 541021\n",
      "500th Most Frequent Word: girl, # of times Seen: 401\n",
      "1000th Most Frequent Word: view, # of times Seen: 1108\n",
      "5000th Most Frequent Word: remotely, # of times Seen: 10\n"
     ]
    }
   ],
   "source": [
    "calcStats(df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_freq = frequentWords(df_yelp)\n",
    "#df_yelp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Most Frequent Words: \n",
      "          Word  Document_Frequency  Word_Count\n",
      "0          the                8530       65257\n",
      "1          and                8273       41324\n",
      "2            a                8045       39029\n",
      "3            i                7631       37811\n",
      "4           to                7425       27964\n",
      "5           is                7104       25401\n",
      "6           it                6884       22204\n",
      "7          not                6543       17225\n",
      "8           of                6501       19543\n",
      "9          for                6272       14782\n",
      "10          wa                6043       23347\n",
      "11          in                5966       13861\n",
      "12         but                5559       10988\n",
      "13        this                5389       10146\n",
      "14        have                5208       10295\n",
      "15        that                5081       11955\n",
      "16        with                4983       10426\n",
      "17          my                4871       10234\n",
      "18        food                4811        8062\n",
      "19          on                4709        8744\n",
      "20        they                4551        9356\n",
      "21       place                4327        7105\n",
      "22         you                4267        9719\n",
      "23        good                4112        6747\n",
      "24         are                4032        7253\n",
      "25         had                3990        7190\n",
      "26          at                3853        6410\n",
      "27          so                3738        6348\n",
      "28          be                3546        5384\n",
      "29        were                3468        7066\n",
      "30          we                3458       10171\n",
      "31       there                3319        5285\n",
      "32        like                3211        4891\n",
      "33          if                3072        4493\n",
      "34        just                3013        4516\n",
      "35        here                2996        4108\n",
      "36        will                2995        3951\n",
      "37        time                2956        4331\n",
      "38         out                2900        4203\n",
      "39     service                2887        3560\n",
      "40         all                2811        4122\n",
      "41         one                2801        4092\n",
      "42          do                2789        4072\n",
      "43         get                2765        3949\n",
      "44        very                2694        4057\n",
      "45         can                2678        3770\n",
      "46       great                2616        3668\n",
      "47          go                2563        3332\n",
      "48          me                2553        4023\n",
      "49          or                2543        3662\n",
      "50        when                2542        3566\n",
      "51        from                2518        3454\n",
      "52       would                2392        3482\n",
      "53       their                2324        3674\n",
      "54          am                2311        3221\n",
      "55        back                2304        2990\n",
      "56          up                2247        3107\n",
      "57        what                2215        3001\n",
      "58       about                2201        2985\n",
      "59        some                2166        3015\n",
      "60      really                2149        3105\n",
      "61        been                2127        2779\n",
      "62          an                2110        2757\n",
      "63       which                2105        3055\n",
      "64         did                2095        3033\n",
      "65  restaurant                2092        3188\n",
      "66        only                1993        2479\n",
      "67       order                1952        2985\n",
      "68         our                1933        3730\n",
      "69          no                1922        2666\n",
      "70     ordered                1883        2569\n",
      "71        more                1871        2400\n",
      "72       other                1771        2178\n",
      "73         too                1764        2206\n",
      "74          by                1753        2246\n",
      "75        also                1744        2255\n",
      "76     because                1670        2161\n",
      "77        your                1670        2342\n",
      "78        even                1650        2061\n",
      "79         got                1595        2167\n",
      "80        menu                1576        2127\n",
      "81     chicken                1562        2558\n",
      "82        than                1531        1844\n",
      "83      little                1528        1982\n",
      "84          ha                1527        1861\n",
      "85        come                1519        1859\n",
      "86        nice                1510        1910\n",
      "87       after                1493        1901\n",
      "88        well                1478        1858\n",
      "89         try                1461        1732\n",
      "90        best                1459        1745\n",
      "91        them                1455        1923\n",
      "92        much                1446        1745\n",
      "93           u                1398        2258\n",
      "94        make                1392        1730\n",
      "95        went                1380        1619\n",
      "96       price                1371        1636\n",
      "97      pretty                1370        1765\n",
      "98         eat                1360        1654\n",
      "99      better                1323        1557\n"
     ]
    }
   ],
   "source": [
    "df_yelp_100 = df_yelp_freq.head(100)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(\"Top 100 Most Frequent Words: \")\n",
    "    print(df_yelp_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Document_Frequency</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>8530</td>\n",
       "      <td>65257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>8273</td>\n",
       "      <td>41324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>8045</td>\n",
       "      <td>39029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>7631</td>\n",
       "      <td>37811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>7425</td>\n",
       "      <td>27964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34358</th>\n",
       "      <td>thingbecause</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34359</th>\n",
       "      <td>overhang</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34360</th>\n",
       "      <td>policing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34361</th>\n",
       "      <td>experimented</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34362</th>\n",
       "      <td>invent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34363 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Document_Frequency  Word_Count\n",
       "0               the                8530       65257\n",
       "1               and                8273       41324\n",
       "2                 a                8045       39029\n",
       "3                 i                7631       37811\n",
       "4                to                7425       27964\n",
       "...             ...                 ...         ...\n",
       "34358  thingbecause                   1           1\n",
       "34359      overhang                   1           2\n",
       "34360      policing                   1           1\n",
       "34361  experimented                   1           1\n",
       "34362        invent                   1           1\n",
       "\n",
       "[34363 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a document frequency of one: 19716\n",
      "\n",
      " 100 words with a document frequency of one: \n",
      "                     Word  Document_Frequency  Word_Count\n",
      "14647                1027                   1           1\n",
      "14648             babelon                   1           3\n",
      "14649          epitomizes                   1           1\n",
      "14650         loudspeaker                   1           1\n",
      "14651            moralize                   1           1\n",
      "14652        amphitheater                   1           1\n",
      "14653               yimmy                   1           1\n",
      "14654             willies                   1           1\n",
      "14655         implication                   1           1\n",
      "14656            breathes                   1           1\n",
      "14657             ruglach                   1           1\n",
      "14658             phyllis                   1           1\n",
      "14659              noches                   1           1\n",
      "14660               hidef                   1           1\n",
      "14661       problemschewy                   1           1\n",
      "14662             admired                   1           1\n",
      "14663           linethere                   1           1\n",
      "14664           descended                   1           1\n",
      "14665          producebut                   1           1\n",
      "14666               harra                   1           2\n",
      "14667          complaines                   1           1\n",
      "14668            cemented                   1           1\n",
      "14669             yalanji                   1           1\n",
      "14670        noncustomers                   1           1\n",
      "14671               foind                   1           1\n",
      "14672    partycelebration                   1           1\n",
      "14673         endorsement                   1           1\n",
      "14674        burgercheese                   1           1\n",
      "14675             whizzed                   1           1\n",
      "14676     englishspeaking                   1           1\n",
      "14677            railroad                   1           2\n",
      "14678                mmmi                   1           1\n",
      "14679              strada                   1           1\n",
      "14680               masse                   1           1\n",
      "14681    taiwanesechinese                   1           1\n",
      "14682              calmly                   1           1\n",
      "14683               aways                   1           1\n",
      "14684            waternow                   1           1\n",
      "14685               lidia                   1           1\n",
      "14686            manzetti                   1           1\n",
      "14687               undue                   1           1\n",
      "14688              dayold                   1           1\n",
      "14689             kingeat                   1           1\n",
      "14690          torringdon                   1           2\n",
      "14691           canceling                   1           1\n",
      "14692             gearing                   1           1\n",
      "14693          yesbecause                   1           1\n",
      "14694       scotsinspired                   1           1\n",
      "14695                 col                   1           1\n",
      "14696              gifted                   1           1\n",
      "14697            wedgwood                   1           2\n",
      "14698               soild                   1           1\n",
      "14699              hommos                   1           1\n",
      "14700             richard                   1           1\n",
      "14701                 tov                   1           1\n",
      "14702               tweet                   1           1\n",
      "14703          tapasstyle                   1           1\n",
      "14704              naming                   1           1\n",
      "14705           phosphate                   1           1\n",
      "14706             stinkys                   1           4\n",
      "14707              chcken                   1           1\n",
      "14708               noosh                   1           1\n",
      "14709            ghannouj                   1           1\n",
      "14710            makeover                   1           1\n",
      "14711        barnightclub                   1           1\n",
      "14712                marj                   1           1\n",
      "14713             restaff                   1           1\n",
      "14714        recollection                   1           1\n",
      "14715             dumbest                   1           1\n",
      "14716              dishis                   1           1\n",
      "14717            misplace                   1           1\n",
      "14718              phobia                   1           1\n",
      "14719              sarnie                   1           1\n",
      "14720            eatenand                   1           1\n",
      "14721      peoplereallydo                   1           1\n",
      "14722          borenstein                   1           1\n",
      "14723              capone                   1           1\n",
      "14724            mehmaybe                   1           1\n",
      "14725              buttah                   1           1\n",
      "14726       documentation                   1           1\n",
      "14727               sousa                   1           2\n",
      "14728                hobo                   1           1\n",
      "14729             oarsoja                   1           1\n",
      "14730                fiji                   1           1\n",
      "14731  thisistheplacetobe                   1           1\n",
      "14732         philosopher                   1           1\n",
      "14733             gunfire                   1           1\n",
      "14734            aquafina                   1           1\n",
      "14735      personalitydid                   1           1\n",
      "14736              trance                   1           1\n",
      "14737                nami                   1           2\n",
      "14738          delibodega                   1           1\n",
      "14739       experiencebut                   1           1\n",
      "14740        bellyperfect                   1           1\n",
      "14741             shopton                   1           1\n",
      "14742        buyonegetone                   1           1\n",
      "14743        respectfully                   1           1\n",
      "14744          accidently                   1           1\n",
      "14745              booboo                   1           1\n",
      "14746            speeding                   1           1\n"
     ]
    }
   ],
   "source": [
    "filtered_df = documentFreq(df_yelp_freq, 1)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(f'Number of words with a document frequency of one: {len(filtered_df)}')\n",
    "    print(\"\")\n",
    "    print(f' 100 words with a document frequency of one: ')\n",
    "    print(filtered_df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headlines Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_2 = openFile(\"headlines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines = createDataFrame(content_2)\n",
    "df_headlines = preprocess(df_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dictionary</th>\n",
       "      <th>fix_lowercase</th>\n",
       "      <th>fix_contractions</th>\n",
       "      <th>fix_punctuation</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>fix_stopwords</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>dictionary_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Worcester breakfast club for veterans gives hu...</td>\n",
       "      <td>worcester breakfast club for veterans gives hu...</td>\n",
       "      <td>worcester breakfast club for veterans gives hu...</td>\n",
       "      <td>worcester breakfast club for veterans gives hu...</td>\n",
       "      <td>worcester breakfast club for veteran give hung...</td>\n",
       "      <td>worcester breakfast club veteran give hunger m...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumpshot Gives Marketers Renewed Visibility In...</td>\n",
       "      <td>jumpshot gives marketers renewed visibility in...</td>\n",
       "      <td>jumpshot gives marketers renewed visibility in...</td>\n",
       "      <td>jumpshot gives marketers renewed visibility in...</td>\n",
       "      <td>jumpshot give marketer renewed visibility into...</td>\n",
       "      <td>jumpshot give marketer renewed visibility paid...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This New Dating App Will Ruin Your Internet Game</td>\n",
       "      <td>this new dating app will ruin your internet game</td>\n",
       "      <td>this new dating app will ruin your internet game</td>\n",
       "      <td>this new dating app will ruin your internet game</td>\n",
       "      <td>this new dating app will ruin your internet game</td>\n",
       "      <td>new dating app ruin internet game</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pay up or face legal action: DBKL</td>\n",
       "      <td>pay up or face legal action: dbkl</td>\n",
       "      <td>pay up or face legal action: dbkl</td>\n",
       "      <td>pay up or face legal action dbkl</td>\n",
       "      <td>pay up or face legal action dbkl</td>\n",
       "      <td>pay face legal action dbkl</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euro up; USD, Pound and Yen down</td>\n",
       "      <td>euro up; usd, pound and yen down</td>\n",
       "      <td>euro up; usd, pound and yen down</td>\n",
       "      <td>euro up usd pound and yen down</td>\n",
       "      <td>euro up usd pound and yen down</td>\n",
       "      <td>euro usd pound yen</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>YWP: Apples</td>\n",
       "      <td>ywp: apples</td>\n",
       "      <td>ywp: apples</td>\n",
       "      <td>ywp apples</td>\n",
       "      <td>ywp apple</td>\n",
       "      <td>ywp apple</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>Microsoft Corporation's Latest Android App Cou...</td>\n",
       "      <td>microsoft corporation's latest android app cou...</td>\n",
       "      <td>microsoft corporation is latest android app co...</td>\n",
       "      <td>microsoft corporation is latest android app co...</td>\n",
       "      <td>microsoft corporation is latest android app co...</td>\n",
       "      <td>microsoft corporation latest android app could...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>Crumbs! Use your loaf and save dough by recycl...</td>\n",
       "      <td>crumbs! use your loaf and save dough by recycl...</td>\n",
       "      <td>crumbs! use your loaf and save dough by recycl...</td>\n",
       "      <td>crumbs use your loaf and save dough by recycli...</td>\n",
       "      <td>crumb use your loaf and save dough by recyclin...</td>\n",
       "      <td>crumb use loaf save dough recycling bread say ...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>New Living Wage 'to benefit one-in-three worki...</td>\n",
       "      <td>new living wage 'to benefit one-in-three worki...</td>\n",
       "      <td>new living wage  noto benefit one-in-three wor...</td>\n",
       "      <td>new living wage  noto benefit oneinthree worki...</td>\n",
       "      <td>new living wage noto benefit oneinthree workin...</td>\n",
       "      <td>new living wage noto benefit oneinthree workin...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>Amy Kass, Inspirational Teacher Who Treasured ...</td>\n",
       "      <td>amy kass, inspirational teacher who treasured ...</td>\n",
       "      <td>amy kass, inspirational teacher who treasured ...</td>\n",
       "      <td>amy kass inspirational teacher who treasured a...</td>\n",
       "      <td>amy ka inspirational teacher who treasured a h...</td>\n",
       "      <td>amy ka inspirational teacher treasured humanis...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               dictionary  \\\n",
       "0       Worcester breakfast club for veterans gives hu...   \n",
       "1       Jumpshot Gives Marketers Renewed Visibility In...   \n",
       "2        This New Dating App Will Ruin Your Internet Game   \n",
       "3                       Pay up or face legal action: DBKL   \n",
       "4                        Euro up; USD, Pound and Yen down   \n",
       "...                                                   ...   \n",
       "499995                                        YWP: Apples   \n",
       "499996  Microsoft Corporation's Latest Android App Cou...   \n",
       "499997  Crumbs! Use your loaf and save dough by recycl...   \n",
       "499998  New Living Wage 'to benefit one-in-three worki...   \n",
       "499999  Amy Kass, Inspirational Teacher Who Treasured ...   \n",
       "\n",
       "                                            fix_lowercase  \\\n",
       "0       worcester breakfast club for veterans gives hu...   \n",
       "1       jumpshot gives marketers renewed visibility in...   \n",
       "2        this new dating app will ruin your internet game   \n",
       "3                       pay up or face legal action: dbkl   \n",
       "4                        euro up; usd, pound and yen down   \n",
       "...                                                   ...   \n",
       "499995                                        ywp: apples   \n",
       "499996  microsoft corporation's latest android app cou...   \n",
       "499997  crumbs! use your loaf and save dough by recycl...   \n",
       "499998  new living wage 'to benefit one-in-three worki...   \n",
       "499999  amy kass, inspirational teacher who treasured ...   \n",
       "\n",
       "                                         fix_contractions  \\\n",
       "0       worcester breakfast club for veterans gives hu...   \n",
       "1       jumpshot gives marketers renewed visibility in...   \n",
       "2        this new dating app will ruin your internet game   \n",
       "3                       pay up or face legal action: dbkl   \n",
       "4                        euro up; usd, pound and yen down   \n",
       "...                                                   ...   \n",
       "499995                                        ywp: apples   \n",
       "499996  microsoft corporation is latest android app co...   \n",
       "499997  crumbs! use your loaf and save dough by recycl...   \n",
       "499998  new living wage  noto benefit one-in-three wor...   \n",
       "499999  amy kass, inspirational teacher who treasured ...   \n",
       "\n",
       "                                          fix_punctuation  \\\n",
       "0       worcester breakfast club for veterans gives hu...   \n",
       "1       jumpshot gives marketers renewed visibility in...   \n",
       "2        this new dating app will ruin your internet game   \n",
       "3                        pay up or face legal action dbkl   \n",
       "4                          euro up usd pound and yen down   \n",
       "...                                                   ...   \n",
       "499995                                         ywp apples   \n",
       "499996  microsoft corporation is latest android app co...   \n",
       "499997  crumbs use your loaf and save dough by recycli...   \n",
       "499998  new living wage  noto benefit oneinthree worki...   \n",
       "499999  amy kass inspirational teacher who treasured a...   \n",
       "\n",
       "                                               lemmatized  \\\n",
       "0       worcester breakfast club for veteran give hung...   \n",
       "1       jumpshot give marketer renewed visibility into...   \n",
       "2        this new dating app will ruin your internet game   \n",
       "3                        pay up or face legal action dbkl   \n",
       "4                          euro up usd pound and yen down   \n",
       "...                                                   ...   \n",
       "499995                                          ywp apple   \n",
       "499996  microsoft corporation is latest android app co...   \n",
       "499997  crumb use your loaf and save dough by recyclin...   \n",
       "499998  new living wage noto benefit oneinthree workin...   \n",
       "499999  amy ka inspirational teacher who treasured a h...   \n",
       "\n",
       "                                            fix_stopwords  num_stopwords  \\\n",
       "0       worcester breakfast club veteran give hunger m...              2   \n",
       "1       jumpshot give marketer renewed visibility paid...              4   \n",
       "2                       new dating app ruin internet game              3   \n",
       "3                              pay face legal action dbkl              2   \n",
       "4                                      euro usd pound yen              3   \n",
       "...                                                   ...            ...   \n",
       "499995                                          ywp apple              0   \n",
       "499996  microsoft corporation latest android app could...              4   \n",
       "499997  crumb use loaf save dough recycling bread say ...              3   \n",
       "499998  new living wage noto benefit oneinthree workin...              0   \n",
       "499999  amy ka inspirational teacher treasured humanis...              2   \n",
       "\n",
       "        word_count  unique_words  dictionary_percentage  \n",
       "0               10             8               0.800000  \n",
       "1               15            10               0.666667  \n",
       "2                9             6               0.666667  \n",
       "3                7             5               0.714286  \n",
       "4                7             4               0.571429  \n",
       "...            ...           ...                    ...  \n",
       "499995           2             2               1.000000  \n",
       "499996          11             7               0.636364  \n",
       "499997          13            10               0.769231  \n",
       "499998           8             8               1.000000  \n",
       "499999          11             9               0.818182  \n",
       "\n",
       "[500000 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_headlines = analyze(df_headlines)\n",
    "df_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above summarizes much of the data that will be used for the calculations below. The dictionary column is the original text and the subsequent 5 columns display the text changes after applying the normalization technique in the title. The lemmatized text was used for the calculations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paragraphs: 500000\n",
      "Total Words: 4587539\n",
      "Total Unique Words: 3512166\n",
      "500th Most Frequent Word: agreement, # of times Seen: 1320\n",
      "1000th Most Frequent Word: grow, # of times Seen: 5295\n",
      "5000th Most Frequent Word: surrender, # of times Seen: 127\n"
     ]
    }
   ],
   "source": [
    "calcStats(df_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_freq = frequentWords(df_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Most Frequent Words: \n",
      "         Word  Document_Frequency  Word_Count\n",
      "0          to              109786    118824.0\n",
      "1          in               83834     88373.0\n",
      "2         the               72269     83558.0\n",
      "3          of               70060     76129.0\n",
      "4         for               65773     67595.0\n",
      "5          is               56690     61706.0\n",
      "6         and               51216     55417.0\n",
      "7           a               49498     54744.0\n",
      "8          on               40822     41841.0\n",
      "9        with               31320     31812.0\n",
      "10         at               31141     31640.0\n",
      "11        new               26321     26830.0\n",
      "12       2015               20555     20903.0\n",
      "13         by               16766     17339.0\n",
      "14       from               16021     16160.0\n",
      "15      after               12922     12967.0\n",
      "16          u               11875     12001.0\n",
      "17         it               11673     12132.0\n",
      "18     market               11374     13187.0\n",
      "19        not               10430     10635.0\n",
      "20       over                9803      9846.0\n",
      "21        say                9640      9681.0\n",
      "22         be                9184      9280.0\n",
      "23         up                9000      9101.0\n",
      "24       will                8713      8798.0\n",
      "25  announces                8376      8381.0\n",
      "26     global                8018      8109.0\n",
      "27        man                7738      7815.0\n",
      "28     report                7648      7787.0\n",
      "29        day                7622      7754.0\n",
      "30       more                7488      7590.0\n",
      "31        you                7417      7962.0\n",
      "32        out                7289      7338.0\n",
      "33        are                7283      7419.0\n",
      "34  september                7140      7192.0\n",
      "35      first                7078      7127.0\n",
      "36       your                7069      7368.0\n",
      "37      world                7002      7108.0\n",
      "38        get                6771      6813.0\n",
      "39       year                6701      6787.0\n",
      "40        how                6699      6742.0\n",
      "41        win                6437      6479.0\n",
      "42     police                6436      6487.0\n",
      "43       this                6385      6441.0\n",
      "44       week                5951      6048.0\n",
      "45      woman                5791      5932.0\n",
      "46       open                5699      5732.0\n",
      "47     launch                5697      5707.0\n",
      "48      about                5675      5730.0\n",
      "49      video                5455      5523.0\n",
      "50    service                5388      5505.0\n",
      "51     school                5375      5531.0\n",
      "52       show                5368      5444.0\n",
      "53      state                5341      5465.0\n",
      "54       home                5339      5444.0\n",
      "55        top                5174      5228.0\n",
      "56        can                5154      5210.0\n",
      "57       make                5141      5157.0\n",
      "58      share                5133      5242.0\n",
      "59         ha                5001      5043.0\n",
      "60   business                4986      5131.0\n",
      "61       that                4965      5024.0\n",
      "62       back                4924      4961.0\n",
      "63      china                4886      4987.0\n",
      "64         an                4882      4942.0\n",
      "65       take                4832      4836.0\n",
      "66         no                4781      4994.0\n",
      "67       what                4778      4845.0\n",
      "68       into                4759      4770.0\n",
      "69      group                4753      4841.0\n",
      "70        one                4719      4864.0\n",
      "71       time                4702      4777.0\n",
      "72          2                4681      4761.0\n",
      "73    against                4647      4662.0\n",
      "74       city                4588      4702.0\n",
      "75     review                4567      4592.0\n",
      "76   industry                4562      4614.0\n",
      "77        off                4562      4625.0\n",
      "78      stock                4556      4616.0\n",
      "79       have                4535      4594.0\n",
      "80        set                4516      4531.0\n",
      "81        inc                4473      5038.0\n",
      "82       game                4446      4536.0\n",
      "83        his                4391      4523.0\n",
      "84    company                4375      4434.0\n",
      "85       help                4325      4360.0\n",
      "86        who                4312      4433.0\n",
      "87       plan                4238      4275.0\n",
      "88       fall                4234      4252.0\n",
      "89   research                4141      4246.0\n",
      "90       call                4123      4143.0\n",
      "91       best                4103      4172.0\n",
      "92        two                4081      4137.0\n",
      "93        now                4047      4063.0\n",
      "94    million                4046      4103.0\n",
      "95     update                3979      3990.0\n",
      "96    refugee                3897      3938.0\n",
      "97         10                3883      3916.0\n",
      "98          1                3873      3965.0\n",
      "99          i                3829      4240.0\n"
     ]
    }
   ],
   "source": [
    "df_head_100 = df_head_freq.head(100)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(\"Top 100 Most Frequent Words: \")\n",
    "    print(df_head_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a document frequency of one: 85192\n",
      "\n",
      " 100 words with a document frequency of one: \n",
      "                      Word  Document_Frequency  Word_Count\n",
      "80310            prajapati                   1         1.0\n",
      "80311              dinos22                   1         1.0\n",
      "80312              rmb400m                   1         1.0\n",
      "80313            clamoring                   1         1.0\n",
      "80314              reporoa                   1         1.0\n",
      "80315             brazoria                   1         1.0\n",
      "80316                  x5r                   1         1.0\n",
      "80317                 terk                   1         1.0\n",
      "80318              kilcock                   1         1.0\n",
      "80319             firstgen                   1         1.0\n",
      "80320            pulsatile                   1         1.0\n",
      "80321             soccerfa                   1         1.0\n",
      "80322            walkabout                   1         1.0\n",
      "80323              macnica                   1         1.0\n",
      "80324          morgansmith                   1         1.0\n",
      "80325               sivaji                   1         1.0\n",
      "80326                ladwp                   1         1.0\n",
      "80327                  t19                   1         1.0\n",
      "80328              jagaurs                   1         1.0\n",
      "80329          spearheaded                   1         1.0\n",
      "80330           0001059142                   1         1.0\n",
      "80331              sigmoid                   1         1.0\n",
      "80332              ttanmai                   1         1.0\n",
      "80333       octoberdefence                   1         1.0\n",
      "80334                  k43                   1         1.0\n",
      "80335           kishanganj                   1         1.0\n",
      "80336                 awai                   1         1.0\n",
      "80337                saing                   1         1.0\n",
      "80338             arborist                   1         1.0\n",
      "80339           stakelbeck                   1         1.0\n",
      "80340             ameeting                   1         1.0\n",
      "80341           vinography                   1         1.0\n",
      "80342            whirlaway                   1         1.0\n",
      "80343         jammukashmir                   1         1.0\n",
      "80344              piebald                   1         1.0\n",
      "80345              maisano                   1         1.0\n",
      "80346            morrilton                   1         1.0\n",
      "80347              chaebol                   1         1.0\n",
      "80348                viger                   1         1.0\n",
      "80349                ffanx                   1         1.0\n",
      "80350            vivacious                   1         1.0\n",
      "80351              allyson                   1         1.0\n",
      "80352         diverticulum                   1         1.0\n",
      "80353                  lky                   1         1.0\n",
      "80354             paeffgen                   1         1.0\n",
      "80355          vasconcelos                   1         1.0\n",
      "80356               danshi                   1         1.0\n",
      "80357                owasi                   1         1.0\n",
      "80358         arcticuswill                   1         1.0\n",
      "80359                 wera                   1         1.0\n",
      "80360                  mrr                   1         1.0\n",
      "80361          thoughtsvas                   1         1.0\n",
      "80362            insideepa                   1         1.0\n",
      "80363               seylan                   1         1.0\n",
      "80364                  rcw                   1         1.0\n",
      "80365             falconry                   1         1.0\n",
      "80366                4door                   1         1.0\n",
      "80367             blackett                   1         1.0\n",
      "80368                bnmda                   1         1.0\n",
      "80369  workspaceasaservice                   1         1.0\n",
      "80370             edgelits                   1         1.0\n",
      "80371         backtoscotus                   1         1.0\n",
      "80372              lichter                   1         1.0\n",
      "80373           commoncore                   1         1.0\n",
      "80374            northline                   1         1.0\n",
      "80375                 keio                   1         1.0\n",
      "80376        leadershoping                   1         1.0\n",
      "80377               maitua                   1         1.0\n",
      "80378           luminarium                   1         1.0\n",
      "80379              aregret                   1         1.0\n",
      "80380              puentes                   1         1.0\n",
      "80381               vitolo                   1         1.0\n",
      "80382             19452015                   1         1.0\n",
      "80383              exhubby                   1         1.0\n",
      "80384                rafay                   1         1.0\n",
      "80385        melgarcabrera                   1         1.0\n",
      "80386      albuquerquearea                   1         1.0\n",
      "80387            queenston                   1         1.0\n",
      "80388                  rdx                   1         1.0\n",
      "80389                perng                   1         1.0\n",
      "80390               pcfast                   1         1.0\n",
      "80391              kahraba                   1         1.0\n",
      "80392             reverted                   1         1.0\n",
      "80393               firmed                   1         1.0\n",
      "80394             netanyau                   1         1.0\n",
      "80395        interviewbank                   1         1.0\n",
      "80396                dhami                   1         1.0\n",
      "80397   chromatographyhplc                   1         1.0\n",
      "80398           entrenches                   1         1.0\n",
      "80399                  aea                   1         1.0\n",
      "80400               faerie                   1         1.0\n",
      "80401           condfident                   1         1.0\n",
      "80402       chamberbeijing                   1         1.0\n",
      "80403              garnero                   1         1.0\n",
      "80404               elvera                   1         1.0\n",
      "80405           powertheft                   1         1.0\n",
      "80406           fliplifetv                   1         1.0\n",
      "80407               120600                   1         1.0\n",
      "80408           ponderings                   1         1.0\n",
      "80409     diphenylprolinol                   1         1.0\n"
     ]
    }
   ],
   "source": [
    "filtered_df2 = documentFreq(df_head_freq, 1)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(f'Number of words with a document frequency of one: {len(filtered_df2)}')\n",
    "    print(\"\")\n",
    "    print(f' 100 words with a document frequency of one: ')\n",
    "    print(filtered_df2.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Write-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Describe how you normalized the text and determined what a word is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text was normalized first making all the words lowercase, substituting the contractions for the full word group, removing any punctuations and lemmatizing the text using the NLTK WordNetLemmatizer. Multiple columns of preprocessed data output displays how the text changes with each normalization step.  The remaining text was tokenized and each token was considered a word. \n",
    "\n",
    "Additional methods could have been stemming as well as substituting numbers for the written words.  \n",
    "I attempted stemming using the PorterStemmmer however it resulting in overstemming where much of the words were cut off. Although stop words were not removed for this assignment, a column of the text after removing the stopwords were displayed.  Stop words could have been removed be as well as other very common words such as \"when\", \"get\", or \"with\" to allow for more important words to be in our dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) summarize any similarities and differences in the top-100 terms from Yelp and Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities:\n",
    "<br>\n",
    "Since stopwords weren't removed, both datasets had stopwords such as \"the\", \"and\", \"to\", as well as others in the top 100 most frequent words. Most of the words included in the top 100 terms also were words of a shorter length.  Also, both lists of words included terms that were not complete words such as \"ha\" in Headlines and \"col\" in in Yelp.\n",
    "\n",
    "Differences:\n",
    "<br>\n",
    "The top words in Yelp appear to be more food related such as \"food\", \"restaurant\", \"ordered\", \"chicken\", as well as other words. Also the Yelp terms, since they are from reviews, are more descriptive words.  However, the top words in Headlines include more numbers and more nouns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
